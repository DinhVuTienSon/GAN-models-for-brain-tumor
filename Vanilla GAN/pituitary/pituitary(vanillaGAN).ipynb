{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Run all code blocks in this section to build and train a Vanilla GAN model with pituitary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess the dataset\n",
    "\n",
    "dataset3 = tf.keras.preprocessing.image_dataset_from_directory(directory='C:/brain_tumor/version2/classify_dataset/pituitary', label_mode=None, batch_size=8, image_size=(256, 256), shuffle=True, color_mode='grayscale')\n",
    "dataset3 = dataset3.map(lambda x: (x - 127.5) / 127.5)\n",
    "dataset3 = dataset3.cache()\n",
    "dataset3 = dataset3.prefetch(16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the generator\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D, BatchNormalization\n",
    "\n",
    "def generator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(16*16*256, input_dim = 256))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Reshape((16,16,256)))\n",
    "    \n",
    "    #upsampling block 1\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, 4, padding='same', use_bias=False))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    #upsampling block 2\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, 4, padding='same', use_bias=False))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    #upsampling block 3\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(32, 4, padding='same', use_bias=False))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    #convolutional layer, reduce to 1 channel\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Conv2D(1, 4, padding='same', use_bias=False, activation='tanh'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "generator = generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some random noises to test the generator\n",
    "\n",
    "image = generator.predict(np.random.randn(4, 256))\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, img in enumerate(image):\n",
    "    for i in range(4):\n",
    "        ax[idx].imshow(np.squeeze(image[i]), cmap='gray')\n",
    "        ax[idx].title.set_text(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the discriminator\n",
    "\n",
    "def discriminator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    #convolutional block 1\n",
    "    model.add(Conv2D(32, 4, strides=(2,2), padding='same', input_shape=(256, 256, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    #convolutional block 2\n",
    "    model.add(Conv2D(64, 4, strides=(2,2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    #convolutional block 3\n",
    "    model.add(Conv2D(128, 4, strides=(2,2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    #convolutional block 4\n",
    "    model.add(Conv2D(256, 4, strides=(2,2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    #flatten\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "discriminator = discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the discriminator\n",
    "\n",
    "discriminator.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the optimizers and losses\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback\n",
    "import os\n",
    "\n",
    "d_optimizer=Adam(lr=0.00002)\n",
    "g_optimizer=Adam(lr=0.0003)\n",
    "d_loss=BinaryCrossentropy()\n",
    "g_loss=BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process \n",
    "\n",
    "class BrainTumorGAN(Model):\n",
    "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "    \n",
    "    def compile(self, generator_optimizer, discriminator_optimizer, generator_loss, discriminator_loss, *args, **kwargs):\n",
    "        super().compile(*args, **kwargs)\n",
    "        \n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "        self.generator_loss = generator_loss\n",
    "        self.discriminator_loss = discriminator_loss\n",
    "        \n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        noise = tf.random.normal(shape=(batch_size, 256))\n",
    "        \n",
    "        # Train discriminator\n",
    "        with tf.GradientTape() as discriminator_tape:\n",
    "            fake_images = self.generator(noise)\n",
    "            real_output = self.discriminator(real_images, training=True)\n",
    "            fake_output = self.discriminator(fake_images, training=True)\n",
    "            \n",
    "            # Compute discriminator loss\n",
    "            discriminator_real_loss = self.discriminator_loss(tf.ones_like(real_output), real_output)\n",
    "            discriminator_fake_loss = self.discriminator_loss(tf.zeros_like(fake_output), fake_output)\n",
    "            total_discriminator_loss = (discriminator_real_loss + discriminator_fake_loss) / 2\n",
    "        \n",
    "        # Backpropagation for discriminator\n",
    "        discriminator_gradients = discriminator_tape.gradient(total_discriminator_loss, self.discriminator.trainable_variables)\n",
    "        self.discriminator_optimizer.apply_gradients(zip(discriminator_gradients, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Train generator\n",
    "        with tf.GradientTape() as generator_tape:\n",
    "            fake_images = self.generator(noise, training=True)\n",
    "            fake_output = self.discriminator(fake_images, training=True)\n",
    "            \n",
    "            # Compute generator loss\n",
    "            generator_loss = self.generator_loss(tf.ones_like(fake_output), fake_output)\n",
    "        \n",
    "        # Backpropagation for generator\n",
    "        generator_gradients = generator_tape.gradient(generator_loss, self.generator.trainable_variables)\n",
    "        self.generator_optimizer.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))\n",
    "        \n",
    "        return {'discriminator_loss': total_discriminator_loss, 'generator_loss': generator_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a callback after each epoch\n",
    "\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class ModelCallback(Callback):\n",
    "    def __init__(self, number_imgs = 3, latent_dim = 256):\n",
    "        self.number_imgs = number_imgs\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs = 'None'):\n",
    "        random_latent_vectors = tf.random.uniform((self.number_imgs, self.latent_dim))\n",
    "        generated_image = self.model.generator(random_latent_vectors)\n",
    "        generated_image = (generated_image * 127.5) + 127.5\n",
    "        generated_image.numpy()\n",
    "        fig = plt.figure(figsize=(20, 20))\n",
    "        for i in range(self.number_imgs):\n",
    "            plt.subplot(5, 5, i+1)\n",
    "            img = tf.keras.utils.array_to_img(generated_image[i])\n",
    "            # img.save(os.path.join('output_dcgan2', f'generated_img_from150_{epoch}_{i}.png'))\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        # plt.savefig('epoch_{:03d}.png'.format(epoch))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "\n",
    "brain_tumor = BrainTumorGAN(generator, discriminator)\n",
    "brain_tumor.compile(g_optimizer, d_optimizer, g_loss, d_loss)\n",
    "hist = brain_tumor.fit(dataset3, epochs=200, callbacks=[ModelCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Using trained model to generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call trained model\n",
    "\n",
    "from keras.models import load_model\n",
    "load_generator6 = load_model('pitui_model/gen6(1000e).h5')\n",
    "load_discriminator6 = load_model('pitui_model/dis6(1000e).h5')\n",
    "model6 = BrainTumorGAN(load_generator6, load_discriminator6) \n",
    "model6.compile(g_optimizer, d_optimizer, g_loss, d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate images\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "number_generated_imgs = 10 # set the number of images to generate\n",
    "noise = tf.random.normal([number_generated_imgs, 256])\n",
    "g_img = model6.generator(noise)\n",
    "g_img = (g_img * 127.5) + 127.5\n",
    "g_img_numpy = g_img.numpy()\n",
    "save_dir = 'C:/brain_tumor/version2/notebook/vanilla_gan/pituitary/pitui_img'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "# fig, ax = plt.subplots(ncols=4, nrows=4, figsize=(20, 20))\n",
    "for i in range(number_generated_imgs):\n",
    "    # ax[i // 4][i % 4].imshow(g_img_numpy[i, :, :, 0], cmap='gray') \n",
    "    # ax[i // 4][i % 4].axis('off')\n",
    "    file_path = os.path.join(save_dir, f'images_14_{i}_1.png')\n",
    "    tf.keras.preprocessing.image.save_img(file_path, g_img_numpy[i])\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3: Evaluate generated images by metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSIM metric\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "real_images_folder = 'C:/brain_tumor/version2/notebook/classify_dataset/pituitary'\n",
    "generated_images_folder = 'C:/brain_tumor/version2/notebook/vanilla_gan/pituitary/pitui_img'\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    image_paths = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            image_paths.append(img_path)\n",
    "    return images, image_paths\n",
    "\n",
    "real_images, real_image_paths = load_images_from_folder(real_images_folder)\n",
    "\n",
    "generated_images, generated_image_paths = load_images_from_folder(generated_images_folder)\n",
    "\n",
    "highest_ssim_values = []\n",
    "\n",
    "for gen_img, gen_img_path in zip(generated_images, generated_image_paths):\n",
    "    max_ssim = -1\n",
    "    best_real_img_path = None\n",
    "    for real_img, real_img_path in zip(real_images, real_image_paths):\n",
    "        if gen_img.shape != real_img.shape:\n",
    "            real_img = cv2.resize(real_img, (gen_img.shape[1], gen_img.shape[0]))\n",
    "\n",
    "        score, _ = ssim(gen_img, real_img, full=True)\n",
    "        if score > max_ssim:\n",
    "            max_ssim = score\n",
    "            best_real_img_path = real_img_path\n",
    "    \n",
    "    highest_ssim_values.append((gen_img_path, max_ssim, best_real_img_path))\n",
    "\n",
    "# Sort the highest SSIM values in descending order\n",
    "highest_ssim_values.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Calculate the average SSIM value\n",
    "total_ssim = sum(value for _, value, _ in highest_ssim_values)\n",
    "average_ssim = total_ssim / len(highest_ssim_values)\n",
    "\n",
    "for gen_img_path, value, real_img_path in highest_ssim_values:\n",
    "    print(f\"Generated image {gen_img_path} - Highest SSIM: {value} - Real image: {real_img_path}\")\n",
    "\n",
    "print(f\"Average SSIM value: {average_ssim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select generated images that have SSIM value > 0.5 \n",
    "# (can store these images to another folder and apply PSNR metric to select imgs have both SSIM > 0,5 and PSNR > 30)\n",
    "\n",
    "for value in highest_ssim_values:\n",
    "    if value[1] > 0.5:\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSNR metric\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "real_images_folder = 'C:/brain_tumor/version2/notebook/classify_dataset/pituitary'\n",
    "generated_images_folder = 'C:/brain_tumor/version2/notebook/vanilla_gan/pituitary/pitui_img'\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    image_paths = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            image_paths.append(img_path)\n",
    "    return images, image_paths\n",
    "\n",
    "def compute_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    PIXEL_MAX = 255.0\n",
    "    return 20 * np.log10(PIXEL_MAX / np.sqrt(mse))\n",
    "\n",
    "real_images, real_image_paths = load_images_from_folder(real_images_folder)\n",
    "generated_images, generated_image_paths = load_images_from_folder(generated_images_folder)\n",
    "\n",
    "highest_psnr_values = []\n",
    "\n",
    "for gen_img, gen_img_path in zip(generated_images, generated_image_paths):\n",
    "    max_psnr = -1\n",
    "    best_real_img_path = None\n",
    "    for real_img, real_img_path in zip(real_images, real_image_paths):\n",
    "        if gen_img.shape != real_img.shape:\n",
    "            real_img = cv2.resize(real_img, (gen_img.shape[1], gen_img.shape[0]))\n",
    "\n",
    "        score = compute_psnr(gen_img, real_img)\n",
    "        if score > max_psnr:\n",
    "            max_psnr = score\n",
    "            best_real_img_path = real_img_path\n",
    "    \n",
    "    highest_psnr_values.append((gen_img_path, max_psnr, best_real_img_path))\n",
    "\n",
    "# Sort the highest PSNR values in descending order\n",
    "highest_psnr_values.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Calculate the average PSNR value\n",
    "total_psnr = sum(value for _, value, _ in highest_psnr_values)\n",
    "average_psnr = total_psnr / len(highest_psnr_values)\n",
    "\n",
    "for gen_img_path, value, real_img_path in highest_psnr_values:\n",
    "    print(f\"Generated image {gen_img_path} - Highest PSNR: {value} - Real image: {real_img_path}\")\n",
    "\n",
    "print(f\"Average PSNR value: {average_psnr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select generated images that have PSNR value > 30 dB\n",
    "\n",
    "for value in highest_psnr_values:\n",
    "    if value[1] > 30:\n",
    "        print(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
