{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Run all code blocks in this section to build and train a Vanilla GAN model with pituitary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess the dataset\n",
    "\n",
    "dataset3 = tf.keras.preprocessing.image_dataset_from_directory(directory='C:/brain_tumor/version2/classify_dataset/meningioma', label_mode=None, batch_size=8, image_size=(256, 256), shuffle=True, color_mode='grayscale')\n",
    "dataset3 = dataset3.map(lambda x: (x - 127.5) / 127.5)\n",
    "dataset3 = dataset3.cache()\n",
    "dataset3 = dataset3.prefetch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the generator\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import BatchNormalization, Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D, Conv2DTranspose\n",
    "from keras.models import Sequential\n",
    "\n",
    "def dc_generator():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(16*16*256, input_dim=256))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((16, 16, 256)))\n",
    "    \n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', use_bias=False))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', use_bias=False))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2DTranspose(32, (4,4), strides=(2,2), padding='same', use_bias=False))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2DTranspose(1, (4,4), strides=(2,2), padding='same',use_bias=False, activation='tanh'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "dc_generator = dc_generator()\n",
    "dc_generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some random noises to test the generator\n",
    "\n",
    "image_dc = dc_generator.predict(np.random.randn(4, 256))\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, img in enumerate(image_dc):\n",
    "    for i in range(4):\n",
    "        ax[idx].imshow(np.squeeze(image_dc[i]), cmap='gray')\n",
    "        ax[idx].title.set_text(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the discriminator\n",
    "\n",
    "def dc_discriminator(input=(256,256,1)):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (4,4), strides=(2, 2), padding='same',input_shape=input))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Conv2D(64, (4,4), strides=(2, 2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Conv2D(128, (4,4), strides=(2, 2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Conv2D(256, (4,4), strides=(2, 2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "dc_discriminator = dc_discriminator()\n",
    "dc_discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the discriminator\n",
    "\n",
    "dc_discriminator.predict(image_dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the optimizers and losses\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback\n",
    "import os\n",
    "\n",
    "dc_d_optimizer=Adam(lr=0.00003)\n",
    "dc_g_optimizer=Adam(lr=0.0002)\n",
    "d_loss=BinaryCrossentropy()\n",
    "g_loss=BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process \n",
    "\n",
    "class DCGAN(Model):\n",
    "    def __init__(self, dc_generator, dc_discriminator):\n",
    "        super().__init__()\n",
    "        self.dc_generator = dc_generator\n",
    "        self.dc_discriminator = dc_discriminator\n",
    "    #     self.g_loss_metric = keras.metrics.Mean(name='g_loss')\n",
    "    #     self.d_loss_metric = keras.metrics.Mean(name='d_loss')\n",
    "    \n",
    "    def compile(self, dc_g_optimizer, dc_d_optimizer, d_loss, g_loss):\n",
    "        super(DCGAN, self).compile()\n",
    "        self.dc_g_optimizer = dc_g_optimizer\n",
    "        self.dc_d_optimizer = dc_d_optimizer\n",
    "        self.d_loss = d_loss\n",
    "        self.g_loss = g_loss\n",
    "        # self.loss_fn = loss_fn\n",
    "        \n",
    "    def train_step(self, real_images):\n",
    "        # get batch size from the data\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        # generate random noise\n",
    "        random_noise = tf.random.normal(shape=(batch_size, 256))\n",
    "        \n",
    "        # train the discriminator with real (1) and fake (0) images\n",
    "        with tf.GradientTape() as tape:\n",
    "            # compute loss on real images\n",
    "            pred_real = self.dc_discriminator(real_images, training=True)\n",
    "            # generate real image labels\n",
    "            real_labels = tf.ones((batch_size, 1))\n",
    "            # label smoothing\n",
    "            real_labels -= 0.15 * tf.random.uniform(tf.shape(real_labels))\n",
    "            d_loss_real = self.d_loss(real_labels, pred_real)\n",
    "            \n",
    "            # compute loss on fake images\n",
    "            fake_images = self.dc_generator(random_noise)\n",
    "            pred_fake = self.dc_discriminator(fake_images, training=True)\n",
    "            # generate fake labels\n",
    "            fake_labels = tf.zeros((batch_size, 1))\n",
    "            fake_labels += 0.15 * tf.random.uniform(tf.shape(fake_labels))\n",
    "            d_loss_fake = self.d_loss(fake_labels, pred_fake)\n",
    "            \n",
    "            # total discriminator loss\n",
    "            dc_d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "            \n",
    "        # compute discriminator gradients\n",
    "        gradients = tape.gradient(dc_d_loss, self.dc_discriminator.trainable_variables)\n",
    "        # update the gradients\n",
    "        self.dc_d_optimizer.apply_gradients(zip(gradients, self.dc_discriminator.trainable_variables))\n",
    "        \n",
    "        # train the generator model\n",
    "        labels = tf.ones((batch_size, 1))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # generate fake images from generator\n",
    "            fake_images = self.dc_generator(random_noise, training=True)\n",
    "            # classify images as real or fake\n",
    "            pred_fake = self.dc_discriminator(fake_images, training=True)\n",
    "            # compute loss\n",
    "            dc_g_loss = self.g_loss(labels, pred_fake)\n",
    "            \n",
    "        # compute gradients\n",
    "        gradients = tape.gradient(dc_g_loss, self.dc_generator.trainable_variables)\n",
    "        # update the gradients\n",
    "        self.dc_g_optimizer.apply_gradients(zip(gradients, self.dc_generator.trainable_variables))\n",
    "        \n",
    "        # # update states for both models\n",
    "        # self.d_loss_metric.update_state(dc_d_loss)\n",
    "        # self.g_loss_metric.update_state(dc_g_loss)\n",
    "        \n",
    "        return {'d_loss': dc_d_loss, 'g_loss': dc_g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a callback after each epoch\n",
    "\n",
    "class DCGANMonitor(Callback):\n",
    "    def __init__(self, num_imgs=3, latent_dim=256):\n",
    "        self.num_imgs = num_imgs\n",
    "        self.latent_dim = latent_dim\n",
    "        # create random noise for generating images\n",
    "        # self.noise = tf.random.normal([4, latent_dim])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # generate the image from noise\n",
    "        noise = tf.random.normal([self.num_imgs, self.latent_dim])\n",
    "        g_img = self.model.dc_generator(noise)\n",
    "        # denormalize the image\n",
    "        g_img = (g_img * 127.5) + 127.5\n",
    "        g_img.numpy()\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 20))\n",
    "        for i in range(self.num_imgs):\n",
    "            plt.subplot(5, 5, i+1)\n",
    "            img = tf.keras.utils.array_to_img(g_img[i])\n",
    "            # img.save(os.path.join('output_dcgan2', f'generated_img_from150_{epoch}_{i}.png'))\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        # plt.savefig('epoch_{:03d}.png'.format(epoch))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "\n",
    "brain_tumor_dc = DCGAN(dc_generator, dc_discriminator)\n",
    "brain_tumor_dc.compile(dc_g_optimizer, dc_d_optimizer, d_loss, g_loss)\n",
    "N_EPOCHS = 200\n",
    "brain_tumor_dc.fit(dataset3, epochs=N_EPOCHS, callbacks=[DCGANMonitor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Using trained model to generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call trained model\n",
    "\n",
    "from keras.models import load_model\n",
    "load_dc_generator12 = load_model('meni_model/gen12(2000e).h5')\n",
    "load_dc_discriminator12 = load_model('meni_model/dis12(2000e).h5')\n",
    "dc_model12 = DCGAN(load_dc_generator12, load_dc_discriminator12)\n",
    "dc_model12.compile(dc_g_optimizer, dc_d_optimizer, d_loss, g_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate images\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "noise = tf.random.normal([50, 256])\n",
    "g_img = dc_model12.dc_generator(noise)\n",
    "g_img = (g_img * 127.5) + 127.5\n",
    "g_img_numpy = g_img.numpy()\n",
    "save_dir = 'C:/brain_tumor/version2/notebook/dc_gan_final/meni/meni_img'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "# fig, ax = plt.subplots(ncols=4, nrows=4, figsize=(20, 20))\n",
    "for i in range(50):\n",
    "    # ax[i // 4][i % 4].imshow(g_img_numpy[i, :, :, 0], cmap='gray') \n",
    "    # ax[i // 4][i % 4].axis('off')\n",
    "    file_path = os.path.join(save_dir, f'images_6_{i}_1.png')\n",
    "    tf.keras.preprocessing.image.save_img(file_path, g_img_numpy[i])\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3: Evaluate generated images by metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSIM metric\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "real_images_folder = 'C:/brain_tumor/version2/notebook/classify_dataset/meningioma'\n",
    "generated_images_folder = 'C:/brain_tumor/version2/notebook/dc_gan_final/meni/meni_img'\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    image_paths = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            image_paths.append(img_path)\n",
    "    return images, image_paths\n",
    "\n",
    "real_images, real_image_paths = load_images_from_folder(real_images_folder)\n",
    "\n",
    "generated_images, generated_image_paths = load_images_from_folder(generated_images_folder)\n",
    "\n",
    "highest_ssim_values = []\n",
    "\n",
    "for gen_img, gen_img_path in zip(generated_images, generated_image_paths):\n",
    "    max_ssim = -1\n",
    "    best_real_img_path = None\n",
    "    for real_img, real_img_path in zip(real_images, real_image_paths):\n",
    "        if gen_img.shape != real_img.shape:\n",
    "            real_img = cv2.resize(real_img, (gen_img.shape[1], gen_img.shape[0]))\n",
    "\n",
    "        score, _ = ssim(gen_img, real_img, full=True)\n",
    "        if score > max_ssim:\n",
    "            max_ssim = score\n",
    "            best_real_img_path = real_img_path\n",
    "    \n",
    "    highest_ssim_values.append((gen_img_path, max_ssim, best_real_img_path))\n",
    "\n",
    "# Sort the highest SSIM values in descending order\n",
    "highest_ssim_values.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Calculate the average SSIM value\n",
    "total_ssim = sum(value for _, value, _ in highest_ssim_values)\n",
    "average_ssim = total_ssim / len(highest_ssim_values)\n",
    "\n",
    "for gen_img_path, value, real_img_path in highest_ssim_values:\n",
    "    print(f\"Generated image {gen_img_path} - Highest SSIM: {value} - Real image: {real_img_path}\")\n",
    "\n",
    "print(f\"Average SSIM value: {average_ssim}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select generated images that have SSIM value > 0.5 \n",
    "# (can store these images to another folder and apply PSNR metric to select imgs have both SSIM > 0,5 and PSNR > 30)\n",
    "\n",
    "for value in highest_ssim_values:\n",
    "    if value[1] > 0.5:\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSNR metric\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "real_images_folder = 'C:/brain_tumor/version2/notebook/classify_dataset/meningioma'\n",
    "generated_images_folder = 'C:/brain_tumor/version2/notebook/dc_gan_final/meni/meni_img'\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    image_paths = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            image_paths.append(img_path)\n",
    "    return images, image_paths\n",
    "\n",
    "def compute_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    PIXEL_MAX = 255.0\n",
    "    return 20 * np.log10(PIXEL_MAX / np.sqrt(mse))\n",
    "\n",
    "real_images, real_image_paths = load_images_from_folder(real_images_folder)\n",
    "generated_images, generated_image_paths = load_images_from_folder(generated_images_folder)\n",
    "\n",
    "highest_psnr_values = []\n",
    "\n",
    "for gen_img, gen_img_path in zip(generated_images, generated_image_paths):\n",
    "    max_psnr = -1\n",
    "    best_real_img_path = None\n",
    "    for real_img, real_img_path in zip(real_images, real_image_paths):\n",
    "        if gen_img.shape != real_img.shape:\n",
    "            real_img = cv2.resize(real_img, (gen_img.shape[1], gen_img.shape[0]))\n",
    "\n",
    "        score = compute_psnr(gen_img, real_img)\n",
    "        if score > max_psnr:\n",
    "            max_psnr = score\n",
    "            best_real_img_path = real_img_path\n",
    "    \n",
    "    highest_psnr_values.append((gen_img_path, max_psnr, best_real_img_path))\n",
    "\n",
    "# Sort the highest PSNR values in descending order\n",
    "highest_psnr_values.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Calculate the average PSNR value\n",
    "total_psnr = sum(value for _, value, _ in highest_psnr_values)\n",
    "average_psnr = total_psnr / len(highest_psnr_values)\n",
    "\n",
    "for gen_img_path, value, real_img_path in highest_psnr_values:\n",
    "    print(f\"Generated image {gen_img_path} - Highest PSNR: {value} - Real image: {real_img_path}\")\n",
    "\n",
    "print(f\"Average PSNR value: {average_psnr}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select generated images that have PSNR value > 30 dB\n",
    "\n",
    "for value in highest_psnr_values:\n",
    "    if value[1] > 30:\n",
    "        print(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
